{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring $WZ$ Diboson Production with ATLAS Open Data\n",
        "\n",
        "This notebook guides you through the analysis of $WZ$ diboson production, specifically focusing on the final state with three leptons ($WZ → 3l$). Using ATLAS Open Data from proton-proton collisions at the Large Hadron Collider (LHC), you will apply event selection criteria to identify and analyze this process.\n",
        "\n",
        "# ATLAS Open Data\n",
        "\n",
        "ATLAS Open Data provides publicly available datasets recorded by the ATLAS experiment at the LHC. These datasets allow students and researchers to explore real high-energy physics data, applying analysis techniques similar to those used in professional research.\n",
        "\n",
        "# What Are Notebooks?\n",
        "\n",
        "Jupyter notebooks provide an interactive environment for combining live code execution, visualizations. This makes them an ideal platform for conducting and documenting particle physics analyses.\n",
        "\n",
        "# The Goal: Identifying $WZ → 3l$ Events\n",
        "\n",
        "In this analysis, we aim to reconstruct the $WZ$ process where:\n",
        "\n",
        "* A $W$ boson decays into a lepton (electron or muon) and a neutrino.\n",
        "\n",
        "* A $Z$ boson decays into a pair of same-flavor opposite-sign leptons.\n",
        "\n",
        "# Running a Jupyter Notebook\n",
        "\n",
        "To execute all cells, go to the top menu and select Cell -> Run All.\n",
        "\n",
        "To run an individual cell, select Cell -> Run Cells or use Shift+Enter."
      ],
      "metadata": {
        "id": "MRBbtmkVho2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uproot"
      ],
      "metadata": {
        "id": "KHF52WWFjwXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the notebook\n",
        "\n",
        "Cell -> Run All Below\n",
        "\n",
        "\n",
        "this should be done every time you re-open this notebook !\n",
        "\n",
        "We're going to be using a number of tools to help us:\n",
        "\n",
        "* uproot: lets us read .root files typically used in particle physics into data formats used in python\n",
        "\n",
        "* awkward: lets us use efficiently the nested data in columnar format\n",
        "\n",
        "* pandas: lets us store data as dataframes, a format widely used in python\n",
        "\n",
        "* numpy: provides numerical calculations such as histogramming\n",
        "\n",
        "* matplotlib: common tool for making plots, figures, images, visualisations"
      ],
      "metadata": {
        "id": "lc0pu75Ujyxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uproot\n",
        "import awkward as ak\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import AutoMinorLocator # for minor ticks\n",
        "from scipy.optimize import curve_fit\n",
        "import requests\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in sqrt\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in power\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in multiply\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in subtract\")"
      ],
      "metadata": {
        "id": "aDq0f7UOlEDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of Key Parameters\n",
        "In particle physics analyses, various parameters and constants are essential for accurately processing data and interpreting results.\n",
        "\n",
        "* Integrated luminosity (lumi): is a measure of the total amount of data collected by a particle detector over a certain period. It represents the total number of potential collisions that could have occurred in a particle accelerator and is typically measured in inverse femtobarns .\n",
        "Fraction of Events to Process\n",
        "\n",
        "* fraction: this parameter controls what fraction of the available events in the dataset will be processed by the analysis in each iteration."
      ],
      "metadata": {
        "id": "v3zAbMjilIoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lumi = 36000.\n",
        "fraction = 0.1 #for 10 iterations, each one will process 10% of the total data"
      ],
      "metadata": {
        "id": "MqNI0oPalJl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Sample Datasets\n",
        "\n",
        "To analyze $WZ$ production, we use both real and simulated (Monte Carlo) datasets. The datasets are categorized into different groups based on their physics processes:\n",
        "\n",
        "* Data: Real collision data recorded by ATLAS.\n",
        "\n",
        "* $WZ$ signal: Simulated WZ events, our main signal process.\n",
        "\n",
        "* $WZ$ background (processes that can mimic WZ events):\n",
        "\n",
        "  1.   Other diboson processes ($WW, ZZ$)\n",
        "  2.   events from $Z+jets$ and $W+jets$\n",
        "  3.   Top quark pair production and single top production\n",
        "\n",
        "\n",
        "The following dictionary organizes these datasets for processing in the analysis."
      ],
      "metadata": {
        "id": "EYNGuqN-lMXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of samples to be processed\n",
        "samples = {\n",
        "\n",
        "    'data': {'list' : [\n",
        "        'data15_periodJ',\n",
        "        'data16_periodB',\n",
        "        'data16_periodK',\n",
        "        'data16_periodC',\n",
        "        'data15_periodE',\n",
        "        'data16_periodF',\n",
        "        'data16_periodG',\n",
        "        'data16_periodD',\n",
        "        'data15_periodD',\n",
        "        'data15_periodH',\n",
        "        'data15_periodF',\n",
        "        'data15_periodG',\n",
        "        'data16_periodE',\n",
        "        'data16_periodA',\n",
        "        'data16_periodI',\n",
        "        'data16_periodL',\n",
        "        ],},\n",
        "\n",
        "\n",
        "\n",
        "    # Main Monte Carlo (MC) sample: top quark pair production\n",
        "    'wz' : {  'list' : ['mc_700601.Sh_2212_lllv'],},\n",
        "\n",
        "    # Diboson WW and ZZ\n",
        "    'diboson' : {  'list' : [\n",
        "        'mc_700493.Sh_2211_ZqqZll',\n",
        "        'mc_700488.Sh_2211_WlvWqq',\n",
        "        'mc_700492.Sh_2211_WqqZll',\n",
        "        'mc_700600.Sh_2212_llll',\n",
        "        #'mc_700494.Sh_2211_ZbbZll',\n",
        "        ],},\n",
        "\n",
        "    # Z + jets and W + jets\n",
        "    'vjets' : {  'list' : [\n",
        "        'mc_700323.Sh_2211_Zmumu_maxHTpTV2_BFilter',\n",
        "        'mc_700324.Sh_2211_Zmumu_maxHTpTV2_CFilterBVeto',\n",
        "        'mc_700325.Sh_2211_Zmumu_maxHTpTV2_CVetoBVeto',\n",
        "        'mc_700470.Sh_2211_Zmumu_maxHTpTV2_m10_40_pT5_BFilter',\n",
        "        'mc_700471.Sh_2211_Zmumu_maxHTpTV2_m10_40_pT5_CFilterBVeto',\n",
        "        'mc_700472.Sh_2211_Zmumu_maxHTpTV2_m10_40_pT5_CVetoBVeto',\n",
        "        'mc_700320.Sh_2211_Zee_maxHTpTV2_BFilter',\n",
        "        'mc_700321.Sh_2211_Zee_maxHTpTV2_CFilterBVeto',\n",
        "        'mc_700322.Sh_2211_Zee_maxHTpTV2_CVetoBVeto',\n",
        "        'mc_700467.Sh_2211_Zee_maxHTpTV2_m10_40_pT5_BFilter',\n",
        "        'mc_700468.Sh_2211_Zee_maxHTpTV2_m10_40_pT5_CFilterBVeto',\n",
        "        'mc_700469.Sh_2211_Zee_maxHTpTV2_m10_40_pT5_CVetoBVeto',\n",
        "        'mc_700792.Sh_2214_Ztautau_maxHTpTV2_BFilter',\n",
        "        'mc_700793.Sh_2214_Ztautau_maxHTpTV2_CFilterBVeto',\n",
        "        'mc_700794.Sh_2214_Ztautau_maxHTpTV2_CVetoBVeto',\n",
        "\n",
        "        'mc_700341.Sh_2211_Wmunu_maxHTpTV2_BFilter',\n",
        "        'mc_700342.Sh_2211_Wmunu_maxHTpTV2_CFilterBVeto',\n",
        "        #'mc_700343.Sh_2211_Wmunu_maxHTpTV2_CVetoBVeto',\n",
        "        'mc_700344.Sh_2211_Wtaunu_L_maxHTpTV2_BFilter',\n",
        "        'mc_700345.Sh_2211_Wtaunu_L_maxHTpTV2_CFilterBVeto',\n",
        "        #'mc_700346.Sh_2211_Wtaunu_L_maxHTpTV2_CVetoBVeto',\n",
        "        'mc_700347.Sh_2211_Wtaunu_H_maxHTpTV2_BFilter',\n",
        "        'mc_700348.Sh_2211_Wtaunu_H_maxHTpTV2_CFilterBVeto',\n",
        "        #'mc_700349.Sh_2211_Wtaunu_H_maxHTpTV2_CVetoBVeto',\n",
        "        'mc_700338.Sh_2211_Wenu_maxHTpTV2_BFilter',\n",
        "        'mc_700339.Sh_2211_Wenu_maxHTpTV2_CFilterBVeto',\n",
        "        #'mc_700340.Sh_2211_Wenu_maxHTpTV2_CVetoBVeto',\n",
        "\n",
        "        ],},\n",
        "\n",
        "    # Single top quark production and ttbar samples\n",
        "    'other' : {  'list' : [\n",
        "        'mc_410470.PhPy8EG_A14_ttbar_hdamp258p75_nonallhad',\n",
        "        'mc_410644.PowhegPythia8EvtGen_A14_singletop_schan_lept_top',\n",
        "        'mc_410645.PowhegPythia8EvtGen_A14_singletop_schan_lept_antitop',\n",
        "        'mc_410658.PhPy8EG_A14_tchan_BW50_lept_top',\n",
        "        'mc_410659.PhPy8EG_A14_tchan_BW50_lept_antitop',\n",
        "        'mc_601487.PhPy8EG_A14_tchan_pThard1_lep_antitop',\n",
        "        'mc_601489.PhPy8EG_A14_tchan_pThard1_lep_top',\n",
        "        'mc_601491.PhPy8EG_A14_ttbar_pThard1_dil',\n",
        "        ],},\n",
        "\n",
        "        }"
      ],
      "metadata": {
        "id": "A0F4QrC-lMgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Event Weights in the Analysis\n",
        "To accurately compare simulated events to real collision data, we apply event weights. These weights account for various factors such as:\n",
        "\n",
        "Pileup corrections: Adjustments for additional collisions in the same event.\n",
        "\n",
        "Lepton scale factors: Corrections for electron and muon identification, reconstruction, and trigger efficiencies.\n",
        "\n",
        "Monte Carlo normalization: Scaling based on cross-sections, k-factors, and filtering efficiencies.\n",
        "\n",
        "By applying event weights, we ensure that simulated samples accurately represent the expected event yields in real LHC data. The following function calculates the total event weight used in this analysis."
      ],
      "metadata": {
        "id": "6CYlo_A8lPz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_weight(data):\n",
        "\n",
        "    scale_factors = (data[\"ScaleFactor_PILEUP\"] * data[\"ScaleFactor_ELE\"] * data[\"ScaleFactor_MUON\"] * data[\"ScaleFactor_MLTRIGGER\"])\n",
        "\n",
        "    norm_factor = (data[\"xsec\"] * data[\"kfac\"] * data[\"filteff\"]) / data[\"sum_of_weights\"]\n",
        "\n",
        "    weight = data[\"mcWeight\"] * scale_factors * norm_factor * lumi\n",
        "\n",
        "    return weight"
      ],
      "metadata": {
        "id": "bcLXYA3MlP7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Dilepton Trigger Selection in the Analysis\n",
        "In high-energy physics experiments like ATLAS, triggers play a crucial role in selecting events of interest while efficiently handling large data volumes.\n",
        "\n",
        "For this analysis, we apply dilepton triggers, which are designed to fire when an event contains at least two leptons (electrons or muons) that pass certain kinematic and identification criteria. We consider two categories of dilepton triggers:\n",
        "\n",
        "* Electron-based dilepton triggers (trigDE): Fire when two electrons satisfy the trigger conditions.\n",
        "\n",
        "* Muon-based dilepton triggers (trigDM): Fire when two muons satisfy the trigger conditions."
      ],
      "metadata": {
        "id": "PncXy7GalQpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_trig(trigDE,trigDM):\n",
        "    return trigDE | trigDM"
      ],
      "metadata": {
        "id": "_pB3oUrjlQxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matching Leptons to Dilepton Triggers\n",
        "In this analysis, we use dilepton triggers, which fire when an event contains at least two leptons that satisfy specific selection criteria. However, not all reconstructed leptons in an event necessarily contributed to the trigger decision. To ensure that at least one of the selected leptons was responsible for firing the trigger, we apply a trigger-matching condition.\n",
        "\n",
        "The variable trigML indicates whether a lepton is matched to the dilepton trigger:\n",
        "\n",
        "* trigML = 1 → The lepton is matched to the dilepton trigger.\n",
        "\n",
        "* trigML = 0 → The lepton is not matched to the trigger.\n",
        "\n",
        "To ensure that we only select events where at least one lepton was responsible for firing the dilepton trigger, we apply the following condition:"
      ],
      "metadata": {
        "id": "rlD7q2pdlQ9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_match_MLtrig(trigML):\n",
        "    return trigML==1\n"
      ],
      "metadata": {
        "id": "3BTBGc7nlRGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting Events with Exactly Three Leptons\n",
        "In the $WZ → 3l$ analysis, we focus on events containing exactly three leptons. This requirement is crucial because:\n",
        "\n",
        "The W boson decays into a lepton (electron or muon) and a neutrino.\n",
        "\n",
        "The Z boson decays into a pair of same-flavor, opposite-sign (SFOS) leptons.\n",
        "\n",
        "By enforcing this selection, we remove events with fewer or more leptons, which are unlikely to originate from $WZ$ production. The function below ensures that only events with exactly three leptons are retained for further analysis."
      ],
      "metadata": {
        "id": "1LLUk22WlRPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def three_lep(lep_n):\n",
        "    return lep_n == 3"
      ],
      "metadata": {
        "id": "VrnJpodNlRX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Transverse Energy ($MET$) Requirement\n",
        "In $WZ → 3l$ events, the W boson decays into a lepton and a neutrino. Since neutrinos do not interact with the ATLAS detector, they escape detection, leading to missing transverse energy.\n",
        "\n",
        "To ensure that selected events contain a neutrino, we apply a minimum $MET$ threshold. This cut helps reduce backgrounds from processes without genuine neutrinos, such as $Z+jets$, where $MET$ arises from mismeasurements.\n",
        "\n",
        "The function below applies a $MET$ cut to retain events with significant missing energy, improving the purity of the $WZ$ selection."
      ],
      "metadata": {
        "id": "d7P5WoWhlbSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_met_et(met_et):\n",
        "    return met_et > 3"
      ],
      "metadata": {
        "id": "Y1W_Mw4PlbZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lepton Transverse Momentum ($p_T$) Selection\n",
        "Transverse momentum ($p_T$) is a crucial variable in event selection, as it helps ensure that the leptons are well-reconstructed and distinguishable from, low-energy background particles.\n",
        "\n",
        "For the $WZ → 3l$ analysis, we apply two conditions to the lepton pt:\n",
        "\n",
        "  1. At least three leptons must have $p_T > 20 GeV$ to guarantee all leptons are energetic enough for accurate reconstruction.\n",
        "\n",
        "  2. At least one lepton must have $p_t > 25 GeV$ to ensure a high-momentum lepton, improving trigger efficiency.\n",
        "\n",
        "These cuts enhance the signal purity while maintaining high efficiency for WZ event selection. The function below implements these requirements."
      ],
      "metadata": {
        "id": "28qpva6cldlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_lep_pt(lep_pt):\n",
        "    pt = lep_pt\n",
        "    cut1 = ak.sum(pt > 20, axis=1) >= 3\n",
        "    cut2 = ak.sum(pt > 25, axis=1) >= 1\n",
        "    return cut1 & cut2"
      ],
      "metadata": {
        "id": "yIxKQjQjldsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lepton Identification and Isolation Requirement\n",
        "Lepton Isolation Selection (Track and Calorimeter)\n",
        "\n",
        "In electroweak processes like WZ → 3ℓ, leptons are typically isolated from other particle activity,\n",
        "whereas background leptons (e.g. from heavy flavor decays or jets faking leptons) are often surrounded by nearby tracks or energy deposits. To distinguish these, we apply two types of isolation cuts:\n",
        " 1. **Track Isolation** (`pt_cone`): Measures the sum of transverse momenta of tracks around the lepton.\n",
        " 2. **Calorimeter Isolation** (`et_cone`): Measures the sum of transverse energy deposits in the calorimeter near the lepton.\n",
        "\n",
        "For both types, we use the ratio of isolation energy to lepton transverse momentum (`iso / pt`).\n",
        "A ratio below 0.15 indicates the lepton is well-isolated.\n",
        "In each case, the event is kept only if **more than two leptons** satisfy `iso / pt < 0.15`.\n",
        "This ensures that all three leptons in the WZ → 3ℓ event are likely prompt, clean, and isolated.\n"
      ],
      "metadata": {
        "id": "QkdnKKQllgDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_track_iso(pt_cone, pt):\n",
        "    cut = ak.sum(np.abs(pt_cone/pt) < 0.15, axis=1) > 2\n",
        "    return cut"
      ],
      "metadata": {
        "id": "oXl2k5IWlgLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_cal_iso(et_cone, pt):\n",
        "    cut = ak.sum(np.abs(et_cone/pt) < 0.15, axis=1) > 2\n",
        "    return cut"
      ],
      "metadata": {
        "id": "IFgIFsyRlghp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lepton Selection: Tight ID with Loose Isolation\n",
        "\n",
        "In this step, we apply a combined selection that uses:\n",
        "\n",
        "  1. **Tight Identification (ID):** Ensures leptons are of high quality — consistent with prompt,\n",
        "     well-reconstructed leptons from the primary vertex (not fakes or from hadron decays).\n",
        "  2. **Loose Isolation (iso):** Allows some nearby detector activity around the lepton.\n",
        "\n",
        "This improves efficiency (especially in busy events) while still rejecting most background.\n",
        "\n",
        "The function checks whether each lepton satisfies both conditions (`ID & iso`) and then counts how many such leptons exist in each event.\n",
        "\n",
        "The event is selected only if **more than two leptons** pass both tight ID and loose isolation. This is a common strategy in WZ → 3ℓ analyses, where the goal is to retain high signal efficiency without compromising much on background rejection.\n"
      ],
      "metadata": {
        "id": "5N139ttQlgqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ID_iso_cut(ID,iso):\n",
        "    return ak.sum(ID & iso,axis=1) > 2"
      ],
      "metadata": {
        "id": "JgqeVUlKlgxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Same-Flavor Opposite-Sign (SFOS) Lepton Pair Selection\n",
        "In the $WZ → 3ℓ$ analysis, identifying a same-flavor opposite-sign (SFOS) lepton pair is crucial for reconstructing the Z boson. The $Z$ boson decays into an electron-positron ($e^-e^+$) or muon-antimuon ($μ^-μ^+$) pair, so we need to:\n",
        "\n",
        "\n",
        "*   Select lepton pairs with the same flavor (both electrons or both muons)\n",
        "*   Ensure the selected pair has opposite charges (one positively and one negatively charged lepton)\n",
        "\n",
        "This step is essential to distinguish real $Z$ boson decays from background processes where lepton pairs may arise from other sources. The function below applies these conditions to each event to retain only those with at least one valid SFOS lepton pair."
      ],
      "metadata": {
        "id": "qQL3REoc9SmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sfos_pair(flavor, charge):\n",
        "\n",
        "    # Generate all unique pairs of particles for each event\n",
        "    # Combinations of 2 ensure no particle is compared with itself\n",
        "    f_pairs = ak.combinations(flavor, 2, fields=[\"flavor_0\", \"flavor_1\"])\n",
        "    c_pairs = ak.combinations(charge, 2, fields=[\"charge_0\", \"charge_1\"])\n",
        "\n",
        "    # Same-flavor check: both particles in the pair must be of the same type\n",
        "    same_flavor = f_pairs.flavor_0 == f_pairs.flavor_1\n",
        "\n",
        "    # Opposite-sign check: the charges of the particles must be different\n",
        "    opposite_sign = c_pairs.charge_0 != c_pairs.charge_1\n",
        "\n",
        "    # Find where both conditions are met\n",
        "    sfos = same_flavor & opposite_sign\n",
        "\n",
        "    # We want at least one SFOS pair per event\n",
        "    sfos_cut = ak.any(sfos, axis=1)\n",
        "\n",
        "    return sfos_cut"
      ],
      "metadata": {
        "id": "emfDkxIdpnWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lepton Four-Momentum Packaging\n",
        "For the WZ → 3ℓ analysis, we need to package the leptons into four-momentum vectors to facilitate further calculations, such as reconstructing invariant masses and analyzing kinematic variables. The four-momentum of a particle is defined by its energy ($E$) and momentum components ($p_x, p_y, p_z$).\n",
        "\n",
        "The function below converts the lepton's transverse momentum ($p_t$), pseudorapidity ($η$), and azimuthal angle ($φ$) into the full momentum components in three dimensions ($p_x, p_y, p_z$), and packages this information into a structured format that can be used for further analysis. It also includes the missing transverse energy ($MET$) and $MET$ $phi$, which are important for the $W$ boson reconstruction."
      ],
      "metadata": {
        "id": "iuREoX-KsIgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def package_leptons(flavor, charge, E, pt, eta, phi, met, met_phi):\n",
        "    lep_px = pt * np.cos(phi)\n",
        "    lep_py = pt * np.sin(phi)\n",
        "    lep_pz = pt / np.tan(2.0 * np.arctan(np.exp(-eta)))\n",
        "\n",
        "    leptons = ak.zip({\n",
        "        \"flavor\": flavor,\n",
        "        \"charge\": charge,\n",
        "        \"E\": E,\n",
        "        \"pt\": pt,\n",
        "        \"eta\": eta,\n",
        "        \"phi\": phi,\n",
        "        \"px\": lep_px,\n",
        "        \"py\": lep_py,\n",
        "        \"pz\": lep_pz,\n",
        "        \"met\": met,\n",
        "        \"met_phi\": met_phi,\n",
        "    })\n",
        "\n",
        "    return leptons"
      ],
      "metadata": {
        "id": "sZkubfuz9SwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding the Closest SFOS Pair to the $Z$ Boson Mass\n",
        "Each event may have more than one same-flavor opposite-sign (SFOS) lepton pairs , so selecting the correct SFOS pair that reconstructs the $Z$ boson mass is a critical step. The function below identifies the lepton pair closest to the $Z$ boson mass (around $91 GeV$) from a set of SFOS pairs. This is done by calculating the invariant mass of each pair and selecting the one with the smallest difference from the $Z$ mass.\n",
        "\n",
        "The process involves:\n",
        "\n",
        "\n",
        "1.   Creating lepton pairs: All unique pairs of leptons from the event are generated.\n",
        "2.   Applying the SFOS condition: Only pairs that are of the same flavor (e.g., both electrons or both muons) and opposite sign (i.e., opposite charges) are kept.\n",
        "3. Calculating the invariant mass of each SFOS pair: The four-momentum is used to calculate the invariant mass of each pair.\n",
        "4. Finding the pair closest to the $Z$ boson mass: The mass difference between the calculated invariant mass of each SFOS pair and the nominal $Z$ boson mass is calculated. The pair with the smallest mass difference is selected.\n",
        "5. Returning the selected leptons: The two leptons that form the SFOS pair closest to the $Z$ mass, along with the invariant mass of the pair, are returned.\n",
        "\n"
      ],
      "metadata": {
        "id": "lppUNQp_tI58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest_sfos_pair(leptons):\n",
        "    z_mass = 91.0\n",
        "    z_window = 10.0\n",
        "\n",
        "\n",
        "    # Create all unique lepton pairs\n",
        "    lep_pairs = ak.combinations(leptons, 2, fields=[\"lep1\", \"lep2\"])\n",
        "\n",
        "    # SFOS condition\n",
        "    same_flavor = lep_pairs.lep1.flavor == lep_pairs.lep2.flavor\n",
        "    opposite_sign = lep_pairs.lep1.charge != lep_pairs.lep2.charge\n",
        "    sfos_mask = same_flavor & opposite_sign\n",
        "\n",
        "    # Filter SFOS pairs\n",
        "    sfos_pairs = lep_pairs[sfos_mask]\n",
        "\n",
        "    # Compute invariant mass of SFOS pairs\n",
        "    E_sum = sfos_pairs.lep1.E + sfos_pairs.lep2.E\n",
        "    px_sum = sfos_pairs.lep1.px + sfos_pairs.lep2.px\n",
        "    py_sum = sfos_pairs.lep1.py + sfos_pairs.lep2.py\n",
        "    pz_sum = sfos_pairs.lep1.pz + sfos_pairs.lep2.pz\n",
        "\n",
        "    masses = np.sqrt(E_sum**2 - (px_sum**2 + py_sum**2 + pz_sum**2))\n",
        "\n",
        "    # Compute absolute difference from Z mass\n",
        "    mass_diff = np.abs(masses - z_mass)\n",
        "\n",
        "    # Create a mask for the pair closest to the Z boson mass\n",
        "    closest_mask = mass_diff == ak.min(mass_diff, axis=1)\n",
        "\n",
        "    # Use boolean mask to select the closest SFOS pair\n",
        "    closest_pair = sfos_pairs[closest_mask]\n",
        "    closest_mass = masses[closest_mask]\n",
        "\n",
        "    selected_lep1 = closest_pair.lep1\n",
        "    selected_lep2 = closest_pair.lep2\n",
        "\n",
        "\n",
        "    return selected_lep1, selected_lep2, closest_mass"
      ],
      "metadata": {
        "id": "iQLyd6-2_9LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Z Boson Mass Cut\n",
        "In the analysis of the $WZ → 3l$ process, we need to identify the leptons that are consistent with the decay of a $Z$ boson. The $Z$ boson decays into a pair of same-flavor, opposite-sign leptons, and we expect the invariant mass of this lepton pair to be close to the known mass of the $Z$ boson (around $91 GeV$).\n",
        "\n",
        "To ensure we are selecting lepton pairs consistent with this decay, we apply a mass window cut:\n",
        "\n",
        "We calculate the invariant mass of each lepton pair.\n",
        "\n",
        "We then apply a cut to select only those pairs whose mass falls within the typical $Z$ boson mass window ($81 GeV < mass < 101 GeV$).\n",
        "\n",
        "This cut is crucial for rejecting background events and selecting signal events where the invariant mass of the lepton pair matches the expected mass of the $Z$ boson. The function returns a boolean array indicating whether each event passes this mass cut."
      ],
      "metadata": {
        "id": "oFZpf4wLu1Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def z_boson_cut(closest_mass):\n",
        "    # Z mass window cut: 81 < mass < 101\n",
        "    z_mass_cut_mask = (closest_mass > 81) & (closest_mass < 101)\n",
        "    z_mass_cut = ak.any(z_mass_cut_mask, axis=1)\n",
        "    return z_mass_cut"
      ],
      "metadata": {
        "id": "G1Pyc7ZmAcKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting SFOS Pairs by flavor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "After selecting the closest same-flavor opposite-sign (SFOS) lepton pair, it's useful to categorize these pairs by their lepton flavor (electron or muon). This step ensures we handle the different lepton flavors separately.\n",
        "\n",
        "This function splits the closest SFOS pairs into two categories:\n",
        "\n",
        "1. Electron pairs ($ee$): Pairs where both leptons are electrons (flavor code 11).\n",
        "\n",
        "2. Muon pairs ($μμ$): Pairs where both leptons are muons (flavor code 13).\n",
        "\n",
        "The function performs the following:\n",
        "\n",
        "* It checks whether the lepton pair consists of two electrons or two muons.\n",
        "\n",
        "\n",
        "* It then creates separate arrays for the invariant mass of the SFOS pairs, depending on whether the pair is electron-electron ($e^-e^+$) or muon-muon ($μ^-μ^+$).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vfzT8oLavG4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sfos_pairs(paired_lep1, paired_lep2, closest_mass):\n",
        "\n",
        "    is_ee = (paired_lep1.flavor == 11) & (paired_lep2.flavor == 11)\n",
        "    is_mm = (paired_lep1.flavor == 13) & (paired_lep2.flavor == 13)\n",
        "\n",
        "    closest_ee_mass = ak.mask(closest_mass, is_ee)\n",
        "    closest_mumu_mass = ak.mask(closest_mass, is_mm)\n",
        "\n",
        "    return closest_ee_mass, closest_mumu_mass"
      ],
      "metadata": {
        "id": "14Gviyf09UO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identifying the Unpaired Lepton\n",
        "In events with exactly three leptons, we need to identify which lepton is unpaired, meaning it is not part of the selected same-flavor opposite-sign (SFOS) pair. This step is crucial for reconstructing the $W$ boson, as the unpaired lepton corresponds to the decay product of the $W$ boson, while the SFOS pair corresponds to the $Z$ boson.\n",
        "\n",
        "The function operates as follows:\n",
        "\n",
        "1. It extracts the individual leptons from the set of three leptons in the event.\n",
        "\n",
        "2. It then checks which lepton is not part of the selected SFOS pair by comparing the transverse momentum ($p_T$) of each lepton to those of the two paired leptons.\n",
        "\n",
        "3. The lepton that is not part of the SFOS pair is identified as the unpaired lepton.\n",
        "\n",
        "The function uses the ak.where function to apply these conditions and returns the unpaired lepton, which will later be used for reconstructing the $W$ boson mass."
      ],
      "metadata": {
        "id": "qIprGmWAv80h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_unpaired_lepton(leptons, paired_lep1, paired_lep2):\n",
        "    # Extract individual leptons\n",
        "    lep1, lep2, lep3 = leptons[:, 0], leptons[:, 1], leptons[:, 2]\n",
        "\n",
        "    # Identify the unpaired lepton\n",
        "    unpaired_lepton = ak.where(\n",
        "        (lep1.pt != paired_lep1.pt) & (lep1.pt != paired_lep2.pt), lep1,\n",
        "        ak.where((lep2.pt != paired_lep1.pt) & (lep2.pt != paired_lep2.pt), lep2, lep3)\n",
        "    )\n",
        "\n",
        "    return unpaired_lepton"
      ],
      "metadata": {
        "id": "fSGYoM5lDiuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating the Transverse Mass of the W Boson\n",
        "The transverse mass of the W boson is a key quantity in reconstructing the W boson in $WZ → 3l$ events. This function computes the transverse mass using the unpaired lepton and missing transverse energy ($MET$).\n",
        "\n",
        "  The transverse mass is calculated using the formula:\n",
        "  $m_T^2 = \\left( p_T^{\\text{lep}} + p_T^{\\text{MET}} \\right)^2 - \\left( p_X^{\\text{lep}} + p_X^{\\text{MET}} \\right)^2 - \\left( p_Y^{\\text{lep}} + p_Y^{\\text{MET}} \\right)^2$\n",
        "\n"
      ],
      "metadata": {
        "id": "-rpuW3_MxFIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def w_boson_mass(unpaired_lepton):\n",
        "\n",
        "    # Convert from MeV to GeV\n",
        "    lep_pt = unpaired_lepton.pt\n",
        "    lep_phi = unpaired_lepton.phi\n",
        "    lep_px = unpaired_lepton.px\n",
        "    lep_py = unpaired_lepton.py\n",
        "    met_et = unpaired_lepton.met\n",
        "    met_phi = unpaired_lepton.met_phi\n",
        "    # Calculate the transverse components (px and py) of the lepton and MET\n",
        "    lep_px = lep_pt * np.cos(lep_phi)\n",
        "    lep_py = lep_pt * np.sin(lep_phi)\n",
        "    met_px = met_et * np.cos(met_phi)\n",
        "    met_py = met_et * np.sin(met_phi)\n",
        "\n",
        "    # Calculate the transverse mass of the W boson\n",
        "    w_mt_squared = ((lep_pt + met_et)**2 - ((lep_px + met_px)**2 + (lep_py + met_py)**2))\n",
        "    w_mt_squared = np.maximum(w_mt_squared, 0)  # Set negative values to zero\n",
        "    w_mt = np.sqrt(w_mt_squared)\n",
        "\n",
        "    return w_mt"
      ],
      "metadata": {
        "id": "Pd4AWAXgD5__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# W boson mass cut\n",
        "The function below filters out events where the transverse mass is below a certain threshold ($30 GeV$). This cut ensures that only events with a significant transverse mass are retained for further analysis, improving the signal-to-background ratio."
      ],
      "metadata": {
        "id": "C0wZPupAyekQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def w_boson_mass_cut(w_mt):\n",
        "\n",
        "    w_mass_cut_mask = w_mt > 30\n",
        "    w_mass_cut = ak.any(w_mass_cut_mask, axis=1)\n",
        "\n",
        "    return w_mass_cut"
      ],
      "metadata": {
        "id": "hL4BooydqJQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the W boson by lepton type\n",
        " The function below splits the transverse mass values ($M_T^W$) into two arrays: one for events with a muon as the unpaired lepton and one for events with an electron. This separation allows for more precise analysis of each channel (muon and electron), as their characteristics and response in the detector are distinct.\n",
        "\n"
      ],
      "metadata": {
        "id": "h0krYNtky1Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_by_type(w_mt, unpaired_lepton):\n",
        "\n",
        "    # Create masks for muons and electrons\n",
        "    is_muon = unpaired_lepton.flavor == 13\n",
        "    is_electron = unpaired_lepton.flavor == 11\n",
        "\n",
        "    # Use the mask to filter w_mt for muons and electrons\n",
        "    w_mt_muon = ak.mask(w_mt, is_muon)\n",
        "    w_mt_electron = ak.mask(w_mt, is_electron)\n",
        "\n",
        "    return w_mt_muon, w_mt_electron"
      ],
      "metadata": {
        "id": "jfqJNW_QD9LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing and Event Selection Function**\n",
        "\n",
        "The read_file function is designed to read and process data from a file, apply selection cuts, and return an awkward array containing the events that pass all cuts."
      ],
      "metadata": {
        "id": "3y9E4PCNpfKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(path,sample,loop):\n",
        "    data_all = [] # define empty list to hold all data for this sample\n",
        "\n",
        "    # open the tree using uproot\n",
        "    with uproot.open(path + f\":analysis;1\") as tree:\n",
        "        numevents = tree.num_entries\n",
        "        for data in tree.iterate(\n",
        "            [\n",
        "                \"mcWeight\", \"sum_of_weights\", \"ScaleFactor_ELE\", \"ScaleFactor_MUON\", \"ScaleFactor_MLTRIGGER\",\n",
        "                \"ScaleFactor_PILEUP\", \"xsec\", \"filteff\", \"kfac\", \"trigDE\", \"trigDM\", \"TriggerMatch_DILEPTON\",\n",
        "                \"lep_n\", \"lep_pt\", \"lep_eta\", \"lep_phi\", \"lep_charge\", \"lep_type\", \"met\", \"met_phi\",\n",
        "                \"lep_e\", \"lep_isTightID\", \"lep_isLooseIso\", \"lep_topoetcone20\", \"lep_ptvarcone30\",\n",
        "            ],\n",
        "            library=\"ak\",\n",
        "            entry_start=int(round(numevents * fraction * loop, 0)),\n",
        "            entry_stop=int(round(numevents * fraction * (loop + 1), 0))\n",
        "        ):\n",
        "\n",
        "            # Apply cuts step by step, updating counters as we go\n",
        "            data = data[cut_trig(data.trigDE, data.trigDM)]\n",
        "\n",
        "            data = data[cut_match_MLtrig(data.TriggerMatch_DILEPTON)]\n",
        "\n",
        "            data = data[three_lep(data.lep_n)]\n",
        "\n",
        "            data = data[cut_met_et(data.met)]\n",
        "\n",
        "            data = data[cut_lep_pt(data.lep_pt)]\n",
        "\n",
        "            data = data[ID_iso_cut(data.lep_isTightID, data.lep_isLooseIso)]\n",
        "\n",
        "            data = data[cut_cal_iso(data.lep_topoetcone20, data.lep_pt)]\n",
        "\n",
        "            data = data[cut_track_iso(data.lep_ptvarcone30, data.lep_pt)]\n",
        "\n",
        "\n",
        "            leptons = package_leptons(data.lep_type, data.lep_charge, data.lep_e, data.lep_pt, data.lep_eta, data.lep_phi, data.met, data.met_phi)\n",
        "            sfos_pairs, sfos_mask = find_sfos_pairs(leptons)\n",
        "            sfos_cut = sfos_cut(sfos_mask)\n",
        "            data = data[sfos_cut]\n",
        "            leptons = leptons[sfos_cut]\n",
        "            sfos_pairs = sfos_pairs[sfos_cut]\n",
        "\n",
        "            paired_lep1, paired_lep2, closest_mass = find_closest_sfos_pair(sfos_pairs)\n",
        "            z_mass_cut = z_mass_cut(closest_mass)\n",
        "\n",
        "            paired_lep1 = paired_lep1[z_mass_cut]\n",
        "            paired_lep2 = paired_lep2[z_mass_cut]\n",
        "            closest_mass = closest_mass[z_mass_cut]\n",
        "\n",
        "            ee_mass, mumu_mass = split_sfos_pairs(paired_lep1, paired_lep2, closest_mass)\n",
        "            unpaired_lepton = find_unpaired_lepton(leptons, paired_lep1, paired_lep2)\n",
        "            w_mass_cut, w_mt = w_boson_mass(unpaired_lepton)\n",
        "            w_mt_muon, w_mt_electron = split_by_type(w_mt, unpaired_lepton)\n",
        "\n",
        "            data = data[w_mass_cut]\n",
        "\n",
        "            data['wtmass'] = w_mt[w_mass_cut]\n",
        "            data['zmass'] = closest_mass[w_mass_cut]\n",
        "            data['ee_pair_mass'] = ee_mass[w_mass_cut]\n",
        "            data['mumu_pair_mass'] = mumu_mass[w_mass_cut]\n",
        "            data['wmuon'] = w_mt_muon[w_mass_cut]\n",
        "            data['welectron'] = w_mt_electron[w_mass_cut]\n",
        "\n",
        "            # Compute weights\n",
        "            if 'data' not in sample:\n",
        "                data['weight'] = calc_weight(data)\n",
        "            else:\n",
        "                data['weight'] = ak.zeros_like(data['met'])\n",
        "\n",
        "            data_all.append(data)\n",
        "            del data, z_mass_cut, w_mass_cut  # Free up memory\n",
        "\n",
        "    return ak.concatenate(data_all)"
      ],
      "metadata": {
        "id": "YiBSPvk5pfTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Retrieval in $WZ \\rightarrow 3l$ Analysis**\n",
        "This function is essential for managing the various datasets needed for a thorough analysis. It focuses on actual experimental data collected from proton-proton collisions at the Large Hadron Collider (LHC)."
      ],
      "metadata": {
        "id": "YzzIupuqpfs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_from_files(type_data, loop):\n",
        "\n",
        "    data = [] # define empty list to hold data\n",
        "    for val in samples[type_data]['list']: # loop over each file\n",
        "        if type_data == \"data\":\n",
        "            prefix = \"Data/\"\n",
        "        else: prefix = \"MC/\"\n",
        "        tuple_path = \"root://eosuser.cern.ch//eos/user/e/egramsta/OpenData/FEB2025/exactly3lep/\"\n",
        "        fileString = tuple_path+prefix+val+\".exactly3lep.root\"\n",
        "        data.append(read_file(fileString,val,loop))\n",
        "\n",
        "    return ak.concatenate(data)"
      ],
      "metadata": {
        "id": "2JOjfgs-pf2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing**\n",
        "The analysis function is a versatile tool designed to process data from specific datasets (data_type) in distinct iterations (loop). It efficiently extracts, organizes, and returns data in the form of a Pandas DataFrame, making it ready for further aggregation and analysis."
      ],
      "metadata": {
        "id": "5HceIS45pgHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis(data_type,loop):\n",
        "\n",
        "    # Process data for data_type sample\n",
        "    data = get_data_from_files(data_type,loop)\n",
        "    data_df = pd.DataFrame({\n",
        "            \"Weight\": ak.to_list(data['weight']),\n",
        "            \"missing_Et\": ak.to_list(data['met']),\n",
        "            \"W_tmass\": ak.to_list(data['wtmass']),\n",
        "            \"W_muon\": ak.to_list(data['wmuon']),\n",
        "            \"w_electron\": ak.to_list(data['welectron']),\n",
        "            \"Z_mass\": ak.to_list(data['zmass']),\n",
        "            \"ee_mass\": ak.to_list(data['ee_pair_mass']),\n",
        "            \"mumu_mass\": ak.to_list(data['mumu_pair_mass']),\n",
        "\n",
        "        })\n",
        "\n",
        "    del(data)       # Delete the 'data' dictionary to free up memory\n",
        "    return data_df  # Return the created Pandas DataFrame"
      ],
      "metadata": {
        "id": "Issd7BhwpgOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Looped Data Processing & Performance Tracking\n",
        "This block executes the full analysis in 10 sequential steps, each handling 10% of the dataset (defined by fraction = 0.1).\n",
        "\n",
        "The loops variable controls how many segments of the data are processed after the initial 0th segment (total iterations = loops + 1).\n",
        "\n",
        "In each iteration, the analysis function is called to process a specific chunk, and the resulting DataFrame is appended to the main DataFrame using pd.concat().\n",
        "\n",
        "The start and end time of each iteration are tracked using the time module to monitor performance and give feedback on the runtime.\n",
        "\n",
        "This modular looped approach helps manage memory and enables progress tracking through printed updates showing both time elapsed and percentage of total data processed.\n"
      ],
      "metadata": {
        "id": "hj1loE6Ypudu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loops = 9             # reduce this if you want the code to run quicker (1-9)\n",
        "start = time.time()  # Time at the start of the whole processing\n",
        "\n",
        "main_analysis_df = analysis('data',0)\n",
        "\n",
        "elapsed = time.time() - start  # time after whole processing\n",
        "print(f\"Time taken: {round(elapsed/60,1)} min, {round(fraction*100,1)}%\")  # print total time taken to process every file\n",
        "\n",
        "if loops > 1:\n",
        "    for i in range(loops):\n",
        "      start = time.time()  # Time at the start of the whole processing\n",
        "\n",
        "      main_analysis_df = pd.concat([analysis(\"data\",i+1), main_analysis_df],ignore_index=True)\n",
        "      elapsed = time.time() - start  # time after whole processing\n",
        "      print(f\"Time taken: {round(elapsed/60,1)} min, {round(fraction*(i+2)*100,1)}%\")  # print total time taken to process every file"
      ],
      "metadata": {
        "id": "e4fZ9ixdppB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous shell we handled the real data samples, so in this shell we follow the same process but for the Monte Carlo samples."
      ],
      "metadata": {
        "id": "h20v70nzpo3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loops = 9             # reduce this if you want the code to run quicker (1-9)\n",
        "start = time.time()  # Time at the start of the whole processing\n",
        "\n",
        "wz_data_df = analysis(\"wz\",0)\n",
        "diboson_data_df = analysis(\"diboson\",0)\n",
        "vjets_data_df = analysis(\"vjets\",0)\n",
        "other_data_df = analysis(\"other\",0)\n",
        "\n",
        "elapsed = time.time() - start  # time after whole processing\n",
        "print(f\"Time taken: {round(elapsed/60,1)} min, {round(fraction*100,1)}%\")  # print total time taken to process every file\n",
        "\n",
        "if loops > 1:\n",
        "    for i in range(loops):\n",
        "      start = time.time()  # Time at the start of the whole processing\n",
        "      wz_data_df = pd.concat([analysis(\"wz\", i+1), wz_data_df],ignore_index=True)\n",
        "      diboson_data_df = pd.concat([analysis(\"diboson\",i+1), diboson_data_df],ignore_index=True)\n",
        "      vjets_data_df = pd.concat([analysis(\"vjets\",i+1), vjets_data_df],ignore_index=True)\n",
        "      other_data_df = pd.concat([analysis(\"other\",i+1), other_data_df],ignore_index=True)\n",
        "      elapsed = time.time() - start  # time after whole processing\n",
        "      print(f\"Time taken: {round(elapsed/60,1)} min, {round(fraction*(i+2)*100,1)}%\")  # print total time taken to process every file"
      ],
      "metadata": {
        "id": "fYmp_pM3pvct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plotting**"
      ],
      "metadata": {
        "id": "yZUvXHBjpyKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data(data, xmin, xmax, step_size, x_label, y_label, data_mc, mc_Weight, data_di,\n",
        "              di_Weight, data_vjets, vjets_Weight, data_other, other_Weight):\n",
        "    # Define MC data sets and their properties\n",
        "    datasets = [\n",
        "        {'data': data_vjets, 'weights': vjets_Weight, 'color': 'green', 'label': r'V+jets'},\n",
        "        {'data': data_other, 'weights': other_Weight, 'color': 'orange', 'label': r'OTHER'},\n",
        "        {'data': data_di, 'weights': di_Weight, 'color': 'slateblue', 'label': r'ZZ,WW'},\n",
        "        {'data': data_mc, 'weights': mc_Weight, 'color': 'lightcoral', 'label': r'WZ'},\n",
        "    ]\n",
        "\n",
        "    # Create bin edges\n",
        "    bin_edges = np.arange(xmin, xmax + step_size, step_size)\n",
        "\n",
        "    # Convert data to numpy for histogramming\n",
        "    data_np = ak.to_numpy(data)\n",
        "\n",
        "    # Handle overflow: add events > xmax to the last bin\n",
        "    data_np[data_np > xmax] = xmax\n",
        "\n",
        "    # Histogram the data\n",
        "    data_x, _ = np.histogram(data_np, bins=bin_edges)\n",
        "    data_x_errors = np.sqrt(data_x)  # Statistical error on the data\n",
        "\n",
        "    # Mask bins with zero events\n",
        "    mask_nonzero = data_x > 0\n",
        "    bin_centres = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
        "    masked_centres = bin_centres[mask_nonzero]\n",
        "    masked_data_x = data_x[mask_nonzero]\n",
        "    masked_data_x_errors = data_x_errors[mask_nonzero]\n",
        "\n",
        "    # Create main plot and residual subplot\n",
        "    fig, (main_axes, residual_axes) = plt.subplots(2, 1, figsize=(7, 6), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
        "\n",
        "    # Plot data with error bars, excluding bins with zero events\n",
        "    main_axes.errorbar(x=masked_centres, y=masked_data_x, yerr=masked_data_x_errors,\n",
        "                       fmt='ko', label='Data')\n",
        "\n",
        "    # Adjust MC datasets for overflow handling\n",
        "    for dataset in datasets:\n",
        "        dataset['data'] = ak.to_numpy(dataset['data'])\n",
        "        dataset['data'][dataset['data'] > xmax] = xmax\n",
        "\n",
        "    # Plot the Monte Carlo bars\n",
        "    mc_heights = main_axes.hist([d['data'] for d in datasets], bins=bin_edges,\n",
        "                                weights=[d['weights'] for d in datasets], stacked=True,\n",
        "                                color=[d['color'] for d in datasets], label=[d['label'] for d in datasets])\n",
        "\n",
        "    # Extract the total stacked MC heights from the last array in mc_heights[0]\n",
        "    mc_x_tot = mc_heights[0][-1]  # Total height in each bin after stacking\n",
        "\n",
        "    # Set up main axes\n",
        "    main_axes.set_xlim(left=xmin, right=xmax)\n",
        "    ymax = max(np.max(data_x), np.max(mc_x_tot))\n",
        "    main_axes.set_ylim(0, ymax * 1.4)  # Add 40% headspace\n",
        "\n",
        "    main_axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
        "    main_axes.tick_params(which='both', direction='in', top=True, right=True)\n",
        "    main_axes.set_ylabel(y_label, y=1, horizontalalignment='right')\n",
        "    main_axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
        "\n",
        "    # Add text to the plot\n",
        "    main_axes.text(0.05, 0.93, 'ATLAS Open Data', transform=main_axes.transAxes, fontsize=13)\n",
        "    main_axes.text(0.05, 0.88, 'for education', transform=main_axes.transAxes, style='italic', fontsize=8)\n",
        "    main_axes.text(0.05, 0.82, r'$W Z \\rightarrow \\ell \\ell \\ell ν$', transform=main_axes.transAxes)\n",
        "\n",
        "    main_axes.legend(frameon=False)\n",
        "\n",
        "    # Calculate and plot residuals\n",
        "    ratio = data_x / mc_x_tot  # Residuals: Data / Total Stacked MC\n",
        "    residual_axes.errorbar(bin_centres, ratio, yerr=ratio * data_x_errors / data_x, fmt='ko')\n",
        "    residual_axes.axhline(1, color='r', linestyle='--')\n",
        "    residual_axes.set_ylim(0, 2)\n",
        "    residual_axes.set_xlabel(x_label, fontsize=13, x=1, horizontalalignment='right')\n",
        "    residual_axes.set_ylabel('Ratio (Data/MC)')\n",
        "    residual_axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
        "    residual_axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
        "    residual_axes.tick_params(which='both', direction='in', top=True, right=True)\n",
        "\n",
        "    # Adjust layout\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(hspace=0.05)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "8Swyy2CKpyTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the function to plot the data"
      ],
      "metadata": {
        "id": "943PZYxa1IlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the columns before passing them\n",
        "plot_data(\n",
        "    np.hstack(main_analysis_df[\"missing_Et\"]), 0, 160, 8, r\"$E_T^{\\mathrm{miss}}$\" \"[GeV]\", \"events / bin\",\n",
        "    np.hstack(wz_data_df[\"missing_Et\"]), np.hstack(wz_data_df['Weight']),\n",
        "    np.hstack(diboson_data_df[\"missing_Et\"]), np.hstack(diboson_data_df['Weight']),\n",
        "    np.hstack(vjets_data_df[\"missing_Et\"]), np.hstack(vjets_data_df['Weight']),\n",
        "    np.hstack(other_data_df[\"missing_Et\"]), np.hstack(other_data_df['Weight'])\n",
        ")"
      ],
      "metadata": {
        "id": "oJMJgNY3nwAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data(\n",
        "    np.hstack(main_analysis_df[\"W_tmass\"]), 0, 200, 10, r\"$M_T^{\\mathrm{W}} [GeV]$\", \"events / bin\",\n",
        "    np.hstack(wz_data_df[\"W_tmass\"]), np.hstack(wz_data_df['Weight']),\n",
        "    np.hstack(diboson_data_df[\"W_tmass\"]), np.hstack(diboson_data_df['Weight']),\n",
        "    np.hstack(vjets_data_df[\"W_tmass\"]), np.hstack(vjets_data_df['Weight']),\n",
        "    np.hstack(other_data_df[\"W_tmass\"]), np.hstack(other_data_df['Weight'])\n",
        ")\n",
        "\n",
        "plot_data(\n",
        "    np.hstack(main_analysis_df[\"Z_mass\"]), 75, 105, 1.5, r\"$M_ll [GeV]$\", 'events / bin',\n",
        "    np.hstack(wz_data_df[\"Z_mass\"]), np.hstack(wz_data_df['Weight']),\n",
        "    np.hstack(diboson_data_df[\"Z_mass\"]), np.hstack(diboson_data_df['Weight']),\n",
        "    np.hstack(vjets_data_df[\"Z_mass\"]), np.hstack(vjets_data_df['Weight']),\n",
        "    np.hstack(other_data_df[\"Z_mass\"]), np.hstack(other_data_df['Weight'])\n",
        ")"
      ],
      "metadata": {
        "id": "kcXJK25UHVrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_analysis_df[\"w_muon\"] = ak.to_numpy(ak.flatten(main_analysis_df[\"w_muon\"]))\n",
        "wz_data_df[\"w_muon\"] = ak.to_numpy(ak.flatten(wz_data_df[\"w_muon\"]))\n",
        "diboson_data_df[\"w_muon\"] = ak.to_numpy(ak.flatten(diboson_data_df[\"w_muon\"]))\n",
        "vjets_data_df[\"w_muon\"] = ak.to_numpy(ak.flatten(vjets_data_df[\"w_muon\"]))\n",
        "other_data_df[\"w_muon\"] = ak.to_numpy(ak.flatten(other_data_df[\"w_muon\"]))\n",
        "\n",
        "plot_data(\n",
        "    np.hstack(main_analysis_df[\"w_muon\"]),  # Flatten Z_mass column\n",
        "    0, 200, 10, r\"$W_μ^{\\mathrm{mass}} [GeV]$\", 'events / bin',\n",
        "    np.hstack(wz_data_df[\"w_muon\"]), np.hstack(wz_data_df['Weight']),\n",
        "    np.hstack(diboson_data_df[\"w_muon\"]), np.hstack(diboson_data_df['Weight']),\n",
        "    np.hstack(vjets_data_df[\"w_muon\"]), np.hstack(vjets_data_df['Weight']),\n",
        "    np.hstack(other_data_df[\"w_muon\"]), np.hstack(other_data_df['Weight'])\n",
        ")"
      ],
      "metadata": {
        "id": "FduYP_CinwZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_analysis_df[\"w_electron\"] = ak.to_numpy(ak.flatten(main_analysis_df[\"w_electron\"]))\n",
        "wz_data_df[\"w_electron\"] = ak.to_numpy(ak.flatten(wz_data_df[\"w_electron\"]))\n",
        "diboson_data_df[\"w_electron\"] = ak.to_numpy(ak.flatten(diboson_data_df[\"w_electron\"]))\n",
        "vjets_data_df[\"w_electron\"] = ak.to_numpy(ak.flatten(vjets_data_df[\"w_electron\"]))\n",
        "other_data_df[\"w_electron\"] = ak.to_numpy(ak.flatten(other_data_df[\"w_electron\"]))\n",
        "\n",
        "plot_data(\n",
        "    np.hstack(main_analysis_df[\"w_electron\"]),  # Flatten Z_mass column\n",
        "    0, 200, 10, r\"$W_e^{\\mathrm{mass}} [GeV]$\", 'events / bin',\n",
        "    np.hstack(wz_data_df[\"w_electron\"]), np.hstack(wz_data_df['Weight']),\n",
        "    np.hstack(diboson_data_df[\"w_electron\"]), np.hstack(diboson_data_df['Weight']),\n",
        "    np.hstack(vjets_data_df[\"w_electron\"]), np.hstack(vjets_data_df['Weight']),\n",
        "    np.hstack(other_data_df[\"w_electron\"]), np.hstack(other_data_df['Weight'])\n",
        ")"
      ],
      "metadata": {
        "id": "IaplUQlKnxI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_analysis_df[\"mumu_mass\"] = ak.to_numpy(ak.flatten(main_analysis_df[\"mumu_mass\"]))\n",
        "wz_data_df[\"mumu_mass\"] = ak.to_numpy(ak.flatten(wz_data_df[\"mumu_mass\"]))\n",
        "diboson_data_df[\"mumu_mass\"] = ak.to_numpy(ak.flatten(diboson_data_df[\"mumu_mass\"]))\n",
        "vjets_data_df[\"mumu_mass\"] = ak.to_numpy(ak.flatten(vjets_data_df[\"mumu_mass\"]))\n",
        "other_data_df[\"mumu_mass\"] = ak.to_numpy(ak.flatten(other_data_df[\"mumu_mass\"]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plot_data(\n",
        "    np.hstack(main_analysis_df[\"mumu_mass\"]),  # Flatten Z_mass column\n",
        "    75, 105, 1.5, r\"$M_μμ [GeV]$\", 'events / bin',\n",
        "    np.hstack(wz_data_df[\"mumu_mass\"]), np.hstack(wz_data_df['Weight']),\n",
        "    np.hstack(diboson_data_df[\"mumu_mass\"]), np.hstack(diboson_data_df['Weight']),\n",
        "    np.hstack(vjets_data_df[\"mumu_mass\"]), np.hstack(vjets_data_df['Weight']),\n",
        "    np.hstack(other_data_df[\"mumu_mass\"]), np.hstack(other_data_df['Weight'])\n",
        ")"
      ],
      "metadata": {
        "id": "i7UEW-conxub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_analysis_df[\"ee_mass\"] = ak.to_numpy(ak.flatten(main_analysis_df[\"ee_mass\"]))\n",
        "wz_data_df[\"ee_mass\"] = ak.to_numpy(ak.flatten(wz_data_df[\"ee_mass\"]))\n",
        "diboson_data_df[\"ee_mass\"] = ak.to_numpy(ak.flatten(diboson_data_df[\"ee_mass\"]))\n",
        "vjets_data_df[\"ee_mass\"] = ak.to_numpy(ak.flatten(vjets_data_df[\"ee_mass\"]))\n",
        "other_data_df[\"ee_mass\"] = ak.to_numpy(ak.flatten(other_data_df[\"ee_mass\"]))\n",
        "\n",
        "\n",
        "\n",
        "plot_data(\n",
        "    np.hstack(main_analysis_df[\"ee_mass\"]),  # Flatten Z_mass column\n",
        "    75, 105, 1.5, r\"$M_ee [GeV]$\", 'events / bin',\n",
        "    np.hstack(wz_data_df[\"ee_mass\"]), np.hstack(wz_data_df['Weight']),\n",
        "    np.hstack(diboson_data_df[\"ee_mass\"]), np.hstack(diboson_data_df['Weight']),\n",
        "    np.hstack(vjets_data_df[\"ee_mass\"]), np.hstack(vjets_data_df['Weight']),\n",
        "    np.hstack(other_data_df[\"ee_mass\"]), np.hstack(other_data_df['Weight'])\n",
        ")"
      ],
      "metadata": {
        "id": "CihzwTStnyKK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}